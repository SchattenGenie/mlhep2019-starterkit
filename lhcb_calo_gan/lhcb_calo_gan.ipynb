{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5py import File as HDF5File\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "sns.set()\n",
    "\n",
    "def one_hot(a, num_classes):\n",
    "    return np.squeeze(np.eye(num_classes)[a.reshape(-1)])\n",
    "\n",
    "device = torch.device('cuda:2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = -1\n",
    "\n",
    "data_real = np.load('../real_quan.npz', allow_pickle=True)\n",
    "EnergyDeposit = data_real['EnergyDeposit']\n",
    "EnergyDeposit = np.log1p(EnergyDeposit.reshape(-1, 1, 30, 30)[:N])\n",
    "ParticleMomentum = data_real['ParticleMomentum'][:N]\n",
    "ParticlePoint = data_real['ParticlePoint'][:N, :2]\n",
    "# ParticlePDG = data_real['ParticlePDG']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load it to pytorch `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnergyDeposit = torch.tensor(EnergyDeposit).float()\n",
    "ParticleMomentum = torch.tensor(ParticleMomentum).float()\n",
    "ParticlePoint = torch.tensor(ParticlePoint).float()\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "calo_dataset = utils.TensorDataset(EnergyDeposit, ParticleMomentum, ParticlePoint)\n",
    "calo_dataloader = torch.utils.data.DataLoader(calo_dataset, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for EnergyDeposit_b, ParticleMomentum_b, ParticlePoint_b in calo_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training GAN\n",
    "###### ...is not a simple matter\n",
    "\n",
    "It depends on architecture, loss, instance noise, augmentation and even luck(recommend to take a look https://arxiv.org/pdf/1801.04406.pdf)\n",
    "\n",
    "\n",
    "In this notebook we have prepared some basic parts that you could use for your experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three types of losses for GANs\n",
    "\n",
    "https://medium.com/@jonathan_hui/gan-what-is-wrong-with-the-gan-cost-function-6f594162ce01\n",
    "\n",
    "There were proposed numerous loss functions to train GANs. In this notebook we have implemented three the most popular choices(but feel free to try other variants!):\n",
    "\n",
    "### `KL`:\n",
    "\n",
    "\n",
    "$$\\mathcal{L}_g = \\log(1 - \\mathrm{discriminator}(\\mathrm{gen}))$$\n",
    "\n",
    "$$\\mathcal{L}_d = - \\log(\\mathrm{discriminator}(\\mathrm{gen})) - \\log(1 - \\mathrm{discriminator}(\\mathrm{real}))$$\n",
    "\n",
    "\n",
    "### `REVERSED_KL`\n",
    "\n",
    "$$\\mathcal{L}_g = - \\log(\\mathrm{discriminator}(\\mathrm{gen}))$$\n",
    "\n",
    "$$\\mathcal{L}_d = - \\log(\\mathrm{discriminator}(\\mathrm{gen})) - \\log(1 - \\mathrm{discriminator}(\\mathrm{real}))$$\n",
    "\n",
    "\n",
    "### `WASSERSTEIN`\n",
    "\n",
    "$$\\mathcal{L}_g = - \\mathrm{discriminator}(\\mathrm{gen})$$\n",
    "\n",
    "$$\\mathcal{L}_d = \\mathrm{discriminator}(\\mathrm{gen}) - \\mathrm{discriminator}(\\mathrm{real})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS = ['KL', 'REVERSED_KL', 'WASSERSTEIN']\n",
    "\n",
    "TASK = 'WASSERSTEIN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional things for wasserstein-loss\n",
    "\n",
    "To make `wasserstein`-GAN works we suggest three options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIPSITZ_WEIGHTS = False\n",
    "\n",
    "# https://arxiv.org/abs/1704.00028\n",
    "GRAD_PENALTY = True\n",
    "\n",
    "# https://arxiv.org/abs/1705.09367\n",
    "ZERO_CENTERED_GRAD_PENALTY = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small hack that can speed-up training and improve generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/abs/1610.04490\n",
    "INSTANCE_NOISE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "\n",
    "class GANLosses(object):\n",
    "    def __init__(self, task, device):\n",
    "        self.TASK = task\n",
    "        self.device = device\n",
    "    \n",
    "    def g_loss(self, discrim_output):\n",
    "        eps = 1e-10\n",
    "        if self.TASK == 'KL': \n",
    "            loss = torch.log(1 - discrim_output + eps).mean()    \n",
    "        elif self.TASK == 'REVERSED_KL':\n",
    "            loss = - torch.log(discrim_output + eps).mean()\n",
    "        elif self.TASK == 'WASSERSTEIN':\n",
    "            loss = - discrim_output.mean()\n",
    "        return loss\n",
    "\n",
    "    def d_loss(self, discrim_output_gen, discrim_output_real):\n",
    "        eps = 1e-10\n",
    "        if self.TASK in ['KL', 'REVERSED_KL']: \n",
    "            loss = - torch.log(discrim_output_real + eps).mean() - torch.log(1 - discrim_output_gen + eps).mean()\n",
    "        elif self.TASK == 'WASSERSTEIN':\n",
    "            loss = - (discrim_output_real.mean() - discrim_output_gen.mean())\n",
    "        return loss\n",
    "\n",
    "    def calc_gradient_penalty(self, discriminator, data_gen, inputs_batch, inp_data, lambda_reg = .1):\n",
    "        alpha = torch.rand(inp_data.shape[0], 1).to(self.device)\n",
    "        dims_to_add = len(inp_data.size()) - 2\n",
    "        for i in range(dims_to_add):\n",
    "            alpha = alpha.unsqueeze(-1)\n",
    "        # alpha = alpha.expand(inp_data.size())\n",
    "\n",
    "        interpolates = (alpha * inp_data + ((1 - alpha) * data_gen)).to(self.device)\n",
    "\n",
    "        interpolates.requires_grad = True\n",
    "\n",
    "        disc_interpolates = discriminator(interpolates, inputs_batch)\n",
    "\n",
    "        gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                                        grad_outputs=torch.ones(disc_interpolates.size()).to(self.device),\n",
    "                                        create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_reg\n",
    "        return gradient_penalty\n",
    "    \n",
    "    def calc_zero_centered_GP(self, discriminator, data_gen, inputs_batch, inp_data, gamma_reg = .1):\n",
    "        \n",
    "        local_input = inp_data.clone().detach().requires_grad_(True)\n",
    "        disc_interpolates = discriminator(local_input, inputs_batch)\n",
    "        gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=local_input,\n",
    "                                        grad_outputs=torch.ones(disc_interpolates.size()).to(self.device),\n",
    "                                        create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "        return gamma_reg / 2 * (gradients.norm(2, dim=1) ** 2).mean() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelD, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3)\n",
    "        self.conv4 = nn.Conv2d(64, 32, 3)\n",
    "        \n",
    "        # size\n",
    "        self.fc1 = nn.Linear(2592 + 5, 512) \n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, EnergyDeposit, ParticleMomentum_ParticlePoint):\n",
    "        EnergyDeposit = self.dropout(F.leaky_relu(self.bn1(self.conv1(EnergyDeposit))))\n",
    "        EnergyDeposit = self.dropout(F.leaky_relu(self.bn2(self.conv2(EnergyDeposit))))\n",
    "        EnergyDeposit = F.leaky_relu(self.conv3(EnergyDeposit))\n",
    "        EnergyDeposit = F.leaky_relu(self.conv4(EnergyDeposit)) # 32, 9, 9\n",
    "        EnergyDeposit = EnergyDeposit.view(len(EnergyDeposit), -1)\n",
    "        \n",
    "        t = torch.cat([EnergyDeposit, ParticleMomentum_ParticlePoint], dim=1)\n",
    "        \n",
    "        t = F.leaky_relu(self.fc1(t))\n",
    "        t = F.leaky_relu(self.fc2(t))\n",
    "        t = F.leaky_relu(self.fc3(t))\n",
    "        if TASK == 'WASSERSTEIN':\n",
    "            return self.fc4(t)\n",
    "        else:\n",
    "            return torch.sigmoid(self.fc4(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelGConvTranspose(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        self.z_dim = z_dim\n",
    "        super(ModelGConvTranspose, self).__init__()\n",
    "        self.fc1 = nn.Linear(self.z_dim + 2 + 3, 64)\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, 512)\n",
    "        self.fc4 = nn.Linear(512, 20736)\n",
    "        \n",
    "        self.conv1 = nn.ConvTranspose2d(256, 256, 3, stride=2, output_padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(256, 128, 3)\n",
    "        self.conv3 = nn.ConvTranspose2d(128, 64, 3)\n",
    "        self.conv4 = nn.ConvTranspose2d(64, 32, 3)\n",
    "        self.conv5 = nn.ConvTranspose2d(32, 16, 3)\n",
    "        self.conv6 = nn.ConvTranspose2d(16, 1, 3)\n",
    "        \n",
    "        \n",
    "    def forward(self, z, ParticleMomentum_ParticlePoint):\n",
    "        x = F.leaky_relu(self.fc1(\n",
    "            torch.cat([z, ParticleMomentum_ParticlePoint], dim=1)\n",
    "        ))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = F.leaky_relu(self.fc4(x))\n",
    "        \n",
    "        EnergyDeposit = x.view(-1, 256, 9, 9)\n",
    "        \n",
    "        EnergyDeposit = F.leaky_relu(self.conv1(EnergyDeposit))\n",
    "        EnergyDeposit = F.leaky_relu(self.conv2(EnergyDeposit))\n",
    "        EnergyDeposit = F.leaky_relu(self.conv3(EnergyDeposit))\n",
    "        EnergyDeposit = F.leaky_relu(self.conv4(EnergyDeposit))\n",
    "        EnergyDeposit = F.leaky_relu(self.conv5(EnergyDeposit))\n",
    "        EnergyDeposit = self.conv6(EnergyDeposit)\n",
    "\n",
    "        return EnergyDeposit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISE_DIM = 2\n",
    "\n",
    "discriminator = ModelD().to(device)\n",
    "generator = ModelGConvTranspose(z_dim=NOISE_DIM).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnergyDeposit_b, ParticleMomentum_b, ParticlePoint_b = EnergyDeposit_b.to(device), \\\n",
    "                                                       ParticleMomentum_b.to(device), \\\n",
    "                                                       ParticlePoint_b.to(device)\n",
    "ParticleMomentum_ParticlePoint_b = torch.cat([ParticleMomentum_b.to(device), ParticlePoint_b.to(device)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnergyDeposit_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator(EnergyDeposit_b, ParticleMomentum_ParticlePoint_b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(len(EnergyDeposit_b), NOISE_DIM).to(device)\n",
    "generator(noise, ParticleMomentum_ParticlePoint_b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_dis = 1e-3\n",
    "learning_rate_gen = 1e-3\n",
    "\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=learning_rate_gen, weight_decay=1e-6)\n",
    "d_optimizer = optim.SGD(discriminator.parameters(), lr=learning_rate_dis, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prd_score import compute_prd, compute_prd_from_embedding, _prd_to_f_beta\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_instance_noise(data, std=0.01):\n",
    "    return data + torch.distributions.Normal(0, std).sample(data.shape).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clamp_lower, clamp_upper = -0.01, 0.01\n",
    "\n",
    "def run_training(epochs):\n",
    "\n",
    "    # ===========================\n",
    "    # IMPORTANT PARAMETER:\n",
    "    # Number of D updates per G update\n",
    "    # ===========================\n",
    "    k_d, k_g = 5, 1\n",
    "\n",
    "    gan_losses = GANLosses(TASK, device)\n",
    "    dis_epoch_loss = []\n",
    "    gen_epoch_loss = []\n",
    "    predictions_dis = []\n",
    "    predictions_gen = []\n",
    "    prd_auc = []  \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        first = True\n",
    "        \n",
    "        for EnergyDeposit_b, ParticleMomentum_b, ParticlePoint_b in calo_dataloader:\n",
    "            EnergyDeposit_b, ParticleMomentum_b, ParticlePoint_b = EnergyDeposit_b.to(device), \\\n",
    "                                                                   ParticleMomentum_b.to(device), \\\n",
    "                                                                   ParticlePoint_b.to(device)\n",
    "            ParticleMomentum_ParticlePoint_b = torch.cat([ParticleMomentum_b.to(device), ParticlePoint_b.to(device)], dim=1)\n",
    "            if first:\n",
    "                noise = torch.randn(len(EnergyDeposit_b), NOISE_DIM).to(device)\n",
    "                EnergyDeposit_gen = generator(noise, ParticleMomentum_ParticlePoint_b)\n",
    "                predictions_dis.append(\n",
    "                    list(discriminator(EnergyDeposit_b, ParticleMomentum_ParticlePoint_b).detach().cpu().numpy().ravel())\n",
    "                )\n",
    "\n",
    "                predictions_gen.append(\n",
    "                    list(discriminator(EnergyDeposit_gen, ParticleMomentum_ParticlePoint_b).detach().cpu().numpy().ravel())\n",
    "                )\n",
    "            # Optimize D\n",
    "            for _ in range(k_d):\n",
    "                noise = torch.randn(len(EnergyDeposit_b), NOISE_DIM).to(device)\n",
    "                EnergyDeposit_gen = generator(noise, ParticleMomentum_ParticlePoint_b)\n",
    "    \n",
    "                if INSTANCE_NOISE:\n",
    "                    EnergyDeposit_b = add_instance_noise(EnergyDeposit_b)\n",
    "                    EnergyDeposit_gen = add_instance_noise(EnergyDeposit_gen)\n",
    "                    \n",
    "                loss = gan_losses.d_loss(discriminator(EnergyDeposit_gen, ParticleMomentum_ParticlePoint_b),\n",
    "                                         discriminator(EnergyDeposit_b, ParticleMomentum_ParticlePoint_b))\n",
    "                if GRAD_PENALTY:\n",
    "                    grad_penalty = gan_losses.calc_gradient_penalty(discriminator,\n",
    "                                                                    EnergyDeposit_gen.data,\n",
    "                                                                    ParticleMomentum_ParticlePoint_b,\n",
    "                                                                    EnergyDeposit_b.data)\n",
    "                    loss += grad_penalty\n",
    "                    \n",
    "                elif ZERO_CENTERED_GRAD_PENALTY:\n",
    "                    grad_penalty = gan_losses.calc_zero_centered_GP(discriminator,\n",
    "                                                                    EnergyDeposit_gen.data,\n",
    "                                                                    ParticleMomentum_ParticlePoint_b,\n",
    "                                                                    EnergyDeposit_b.data)\n",
    "                    loss -= grad_penalty\n",
    "\n",
    "                d_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                d_optimizer.step()\n",
    "                if LIPSITZ_WEIGHTS:                    \n",
    "                    [p.data.clamp_(clamp_lower, clamp_upper) for p in discriminator.parameters()]\n",
    "\n",
    "            dis_epoch_loss.append(loss.item())\n",
    "\n",
    "            # Optimize G\n",
    "            for _ in range(k_g):\n",
    "                noise = torch.randn(len(EnergyDeposit_b), NOISE_DIM).to(device)\n",
    "                EnergyDeposit_gen = generator(noise, ParticleMomentum_ParticlePoint_b)\n",
    "                \n",
    "                if INSTANCE_NOISE:\n",
    "                    EnergyDeposit_b = add_instance_noise(EnergyDeposit_b)\n",
    "                    EnergyDeposit_gen = add_instance_noise(EnergyDeposit_gen)\n",
    "                \n",
    "                loss = gan_losses.g_loss(discriminator(EnergyDeposit_gen, ParticleMomentum_ParticlePoint_b))\n",
    "                g_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                g_optimizer.step()\n",
    "                \n",
    "            gen_epoch_loss.append(loss.item())\n",
    "            if first:\n",
    "                precision, recall = compute_prd_from_embedding(\n",
    "                    EnergyDeposit_gen.detach().cpu().numpy().reshape(BATCH_SIZE, -1), \n",
    "                    EnergyDeposit_b.detach().cpu().numpy().reshape(BATCH_SIZE, -1),\n",
    "                    num_clusters=30,\n",
    "                    num_runs=100)\n",
    "                prd_auc.append(auc(precision, recall))\n",
    "                first = False\n",
    "        \n",
    "        clear_output()\n",
    "        plt.figure(figsize=(12, 12))\n",
    "        plt.plot(dis_epoch_loss, label='dis_epoch_loss')\n",
    "        plt.plot(gen_epoch_loss, label='gen_epoch_loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(12, 12))\n",
    "        plt.hist(predictions_dis[-1], bins=100, label='dis_epoch_loss')\n",
    "        plt.hist(predictions_gen[-1], bins=100, label='gen_epoch_loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        print(np.mean(predictions_dis[-1]), np.mean(predictions_gen[-1]))\n",
    "        \n",
    "        plt.figure(figsize=(12, 12))\n",
    "        plt.plot(prd_auc, label='prd_auc')\n",
    "        plt.plot()\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_training(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prd_score import compute_prd, compute_prd_from_embedding, _prd_to_f_beta\n",
    "from sklearn.metrics import auc\n",
    "noise = torch.randn(BATCH_SIZE, NOISE_DIM).to(device)\n",
    "ParticleMomentum_ParticlePoint_b = torch.cat([ParticleMomentum_b.to(device), \n",
    "                                              ParticlePoint_b.to(device)], dim=1)\n",
    "EnergyDeposit_gen = generator(noise, ParticleMomentum_ParticlePoint_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(EnergyDeposit_gen[0, 0].detach().cpu().numpy())\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall = compute_prd_from_embedding(\n",
    "                    EnergyDeposit_gen.detach().cpu().numpy().reshape(BATCH_SIZE, -1), \n",
    "                    EnergyDeposit_b.detach().cpu().numpy().reshape(BATCH_SIZE, -1),\n",
    "                    num_clusters=30,\n",
    "                    num_runs=100)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "plt.step(recall, precision, color='b', alpha=0.2,  where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = np.load('../data_val.npz', allow_pickle=True)\n",
    "\n",
    "ParticleMomentum_val = torch.tensor(data_val['ParticleMomentum']).float()\n",
    "ParticlePoint_val = torch.tensor(data_val['ParticlePoint'][:, :2]).float()\n",
    "ParticleMomentum_ParticlePoint_val = torch.cat([ParticleMomentum_val, ParticlePoint_val], dim=1)\n",
    "noise = torch.randn(len(ParticleMomentum_ParticlePoint_val), NOISE_DIM)\n",
    "EnergyDeposit_val = generator.cpu()(noise, ParticleMomentum_ParticlePoint_val)\n",
    "np.savez_compressed('./data_val_solution.npz', \n",
    "                    EnergyDeposit=EnergyDeposit_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = np.load('../data_test.npz', allow_pickle=True)\n",
    "\n",
    "ParticleMomentum_test = torch.tensor(data_test['ParticleMomentum']).float()\n",
    "ParticlePoint_test = torch.tensor(data_test['ParticlePoint'][:, :2]).float()\n",
    "ParticleMomentum_ParticlePoint_test = torch.cat([ParticleMomentum_test, ParticlePoint_test], dim=1)\n",
    "noise = torch.randn(len(ParticleMomentum_ParticlePoint_test), NOISE_DIM)\n",
    "EnergyDeposit_test = generator.cpu()(noise, ParticleMomentum_ParticlePoint_test)\n",
    "np.savez_compressed('./data_test_solution.npz', \n",
    "                    EnergyDeposit=EnergyDeposit_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
